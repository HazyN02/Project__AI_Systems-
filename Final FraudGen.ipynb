{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d5d48f-d00c-4ebe-bd51-fb00e4d49970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading raw transaction data...\n",
      "[INFO] Dropping 168 cols with >70% missing.\n",
      "[INFO] Numeric cols: 212\n",
      "[INFO] Categorical cols: 13\n",
      "[OK] Processed data saved -> Processed/train_processed.csv\n",
      "[INFO] Fraud rows: 20663, Legit rows: 569877\n",
      "[INFO] CTGAN discrete columns: 13\n",
      "[INFO] Training CTGAN...\n",
      "Epoch 1, Loss G:  0.7285,Loss D: -0.7536\n",
      "Epoch 2, Loss G:  0.1313,Loss D: -1.2873\n",
      "Epoch 3, Loss G: -0.8933,Loss D: -0.9818\n",
      "Epoch 4, Loss G: -0.6472,Loss D: -0.8660\n",
      "Epoch 5, Loss G:  0.1004,Loss D: -1.1312\n",
      "Epoch 6, Loss G: -0.0793,Loss D: -0.7246\n",
      "Epoch 7, Loss G: -0.3549,Loss D: -0.4958\n",
      "Epoch 8, Loss G: -1.9143,Loss D: -0.3350\n",
      "Epoch 9, Loss G: -1.0340,Loss D: -0.7519\n",
      "Epoch 10, Loss G: -1.0568,Loss D: -0.5028\n",
      "Epoch 11, Loss G: -0.7876,Loss D: -0.6470\n",
      "Epoch 12, Loss G: -0.8281,Loss D: -0.7404\n",
      "Epoch 13, Loss G: -0.6912,Loss D: -0.4781\n",
      "Epoch 14, Loss G: -0.5377,Loss D: -0.6898\n",
      "Epoch 15, Loss G: -0.8907,Loss D: -0.8770\n",
      "[OK] CTGAN model saved -> Models\\ctgan_fraudgen_v3.pkl\n",
      "[INFO] Generating synthetic fraud samples...\n",
      "[OK] Synthetic data saved -> Synthetic/synthetic_fraud_v3.csv\n",
      "[OK] Synthetic validation metrics saved.\n",
      "[INFO] Training fraud classifiers (baseline vs synthetic-aug)...\n",
      "[OK] Fraud detection metrics (baseline vs augmented) saved.\n",
      "[INFO] Training IsolationForest anomaly detector...\n",
      "[OK] Anomaly detection metrics saved.\n",
      "[INFO] Saving KDE distribution plots for top numeric features...\n",
      "üéâ DONE. FRAUDGEN v3 PIPELINE FINISHED SUCCESSFULLY.\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# FRAUDGEN v3 ‚Äî FINAL PIPELINE (CPU, CTGAN, VALIDATION, ANOMALY)\n",
    "#############################################################\n",
    "\n",
    "import os, warnings, json, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # non-interactive backend, avoids GUI issues\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    \"RAW_TRANS\": \"IEEE Primary Data/train_transaction.csv\",  # input\n",
    "    \"PROCESSED\": \"Processed/train_processed.csv\",\n",
    "    \"SYNTHETIC\": \"Synthetic/synthetic_fraud_v3.csv\",\n",
    "    \"MODELS_DIR\": \"Models\",\n",
    "    \"REPORTS_DIR\": \"Reports\",\n",
    "    \"PLOTS_DIR\": \"Distribution_Plots\",\n",
    "    \"N_SYNTH\": 200_000,\n",
    "    \"CTGAN_EPOCHS\": 15,\n",
    "    \"CTGAN_BATCH\": 128,\n",
    "    \"CTGAN_PAC\": 1,\n",
    "    \"SEED\": 42,\n",
    "}\n",
    "\n",
    "for d in [\"Processed\", \"Synthetic\", \"Models\", \"Reports\", \"Distribution_Plots\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "SEED = CONFIG[\"SEED\"]\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# 0) PATCH JOBLIB TO FORCE SERIAL (CPU / WINDOWS SAFE)\n",
    "# ============================================================\n",
    "from joblib import Parallel as JoblibParallel\n",
    "\n",
    "class SerialParallel(JoblibParallel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs[\"n_jobs\"] = 1\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "joblib.Parallel = SerialParallel  # override BEFORE importing ctgan\n",
    "\n",
    "# Torch seed (if available)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# CTGAN\n",
    "from ctgan import CTGAN\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) PREPROCESSING\n",
    "# ============================================================\n",
    "def preprocess_transactions():\n",
    "    print(\"[INFO] Loading raw transaction data...\")\n",
    "    df = pd.read_csv(CONFIG[\"RAW_TRANS\"])\n",
    "\n",
    "    if \"isFraud\" not in df.columns:\n",
    "        raise KeyError(\"Column 'isFraud' not found in dataset.\")\n",
    "\n",
    "    # Drop columns with >70% missing\n",
    "    miss_ratio = df.isna().mean()\n",
    "    drop_cols = miss_ratio[miss_ratio > 0.7].index.tolist()\n",
    "    if drop_cols:\n",
    "        print(f\"[INFO] Dropping {len(drop_cols)} cols with >70% missing.\")\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Separate numeric vs categorical based on dtypes\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"float32\", \"int64\", \"int32\"]).columns.tolist()\n",
    "    if \"isFraud\" in numeric_cols:\n",
    "        numeric_cols.remove(\"isFraud\")\n",
    "\n",
    "    cat_cols = [c for c in df.columns if c not in numeric_cols + [\"isFraud\"]]\n",
    "\n",
    "    print(f\"[INFO] Numeric cols: {len(numeric_cols)}\")\n",
    "    print(f\"[INFO] Categorical cols: {len(cat_cols)}\")\n",
    "\n",
    "    # Fill NaNs\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    # Scale numeric with QuantileTransformer\n",
    "    qt = QuantileTransformer(\n",
    "        n_quantiles=min(500, len(df)),\n",
    "        output_distribution=\"normal\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "    df[numeric_cols] = qt.fit_transform(df[numeric_cols])\n",
    "    joblib.dump(qt, os.path.join(CONFIG[\"MODELS_DIR\"], \"quantile_transformer.pkl\"))\n",
    "\n",
    "    # Save processed\n",
    "    df.to_csv(CONFIG[\"PROCESSED\"], index=False)\n",
    "    print(\"[OK] Processed data saved ->\", CONFIG[\"PROCESSED\"])\n",
    "\n",
    "    return df, numeric_cols, cat_cols\n",
    "\n",
    "\n",
    "df, numeric_cols, cat_cols = preprocess_transactions()\n",
    "\n",
    "# Split fraud / non-fraud after preprocessing\n",
    "fraud_df = df[df[\"isFraud\"] == 1].reset_index(drop=True)\n",
    "legit_df = df[df[\"isFraud\"] == 0].reset_index(drop=True)\n",
    "print(f\"[INFO] Fraud rows: {len(fraud_df)}, Legit rows: {len(legit_df)}\")\n",
    "\n",
    "if len(fraud_df) < 500:\n",
    "    print(\"[WARN] Very few fraud samples. CTGAN may be unstable.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) PREPARE DATA FOR CTGAN\n",
    "# ============================================================\n",
    "# CTGAN expects a DataFrame (no target), plus discrete column list\n",
    "\n",
    "# Discrete columns = categorical columns (non-numeric), with >1 unique\n",
    "discrete_cols = []\n",
    "for c in cat_cols:\n",
    "    if fraud_df[c].nunique() > 1:\n",
    "        discrete_cols.append(c)\n",
    "\n",
    "# Ensure discrete cols not empty (CTGAN can handle all continuous, but we keep this for clarity)\n",
    "print(f\"[INFO] CTGAN discrete columns: {len(discrete_cols)}\")\n",
    "\n",
    "# Training data for CTGAN: ONLY fraud rows, NO label\n",
    "ctgan_train = fraud_df.drop(columns=[\"isFraud\"]).copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) TRAIN CTGAN\n",
    "# ============================================================\n",
    "def train_ctgan(train_df, discrete_columns):\n",
    "    print(\"[INFO] Training CTGAN...\")\n",
    "    ctgan = CTGAN(\n",
    "        epochs=CONFIG[\"CTGAN_EPOCHS\"],\n",
    "        batch_size=CONFIG[\"CTGAN_BATCH\"],\n",
    "        pac=CONFIG[\"CTGAN_PAC\"],\n",
    "        verbose=True\n",
    "    )\n",
    "    # Patch transformer to avoid leftover state\n",
    "    ctgan._data_transformer = None\n",
    "    ctgan.fit(train_df, discrete_columns=discrete_columns)\n",
    "    model_path = os.path.join(CONFIG[\"MODELS_DIR\"], \"ctgan_fraudgen_v3.pkl\")\n",
    "    joblib.dump(ctgan, model_path)\n",
    "    print(\"[OK] CTGAN model saved ->\", model_path)\n",
    "    return ctgan\n",
    "\n",
    "\n",
    "ctgan = train_ctgan(ctgan_train, discrete_columns=discrete_cols)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) GENERATE SYNTHETIC FRAUD\n",
    "# ============================================================\n",
    "print(\"[INFO] Generating synthetic fraud samples...\")\n",
    "synthetic = ctgan.sample(CONFIG[\"N_SYNTH\"])\n",
    "synthetic[\"isFraud\"] = 1\n",
    "\n",
    "# Reorder columns to match df\n",
    "cols_order = [c for c in df.columns if c in synthetic.columns]\n",
    "synthetic = synthetic[cols_order]\n",
    "\n",
    "synth_path = CONFIG[\"SYNTHETIC\"]\n",
    "synthetic.to_csv(synth_path, index=False)\n",
    "print(\"[OK] Synthetic data saved ->\", synth_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) SYNTHETIC VALIDATION SYSTEM\n",
    "# ============================================================\n",
    "reports = {}\n",
    "\n",
    "# Use fraud_df (real fraud) vs synthetic (all fraud by design)\n",
    "real_fraud = fraud_df.copy()\n",
    "synthetic_fraud = synthetic.copy()\n",
    "\n",
    "# ---------- KS & Wasserstein for numeric features ----------\n",
    "rows = []\n",
    "for c in numeric_cols:\n",
    "    try:\n",
    "        r = real_fraud[c].astype(float)\n",
    "        s = synthetic_fraud[c].astype(float)\n",
    "        ks = ks_2samp(r, s).statistic\n",
    "        ws = wasserstein_distance(r, s)\n",
    "        rows.append([c, ks, ws])\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "ks_ws_df = pd.DataFrame(rows, columns=[\"Feature\", \"KS\", \"Wasserstein\"])\n",
    "ks_ws_df.to_csv(os.path.join(CONFIG[\"REPORTS_DIR\"], \"validation_ks_wasserstein.csv\"), index=False)\n",
    "\n",
    "reports[\"mean_KS\"] = float(ks_ws_df[\"KS\"].mean())\n",
    "reports[\"mean_Wasserstein\"] = float(ks_ws_df[\"Wasserstein\"].mean())\n",
    "\n",
    "# ---------- PCA distance (distribution overlap check) ----------\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X_real = real_fraud[numeric_cols].values\n",
    "X_synth = synthetic_fraud[numeric_cols].values\n",
    "X_combined = np.vstack([X_real, X_synth])\n",
    "X_pca = pca.fit_transform(X_combined)\n",
    "real_pca = X_pca[: len(X_real)]\n",
    "synth_pca = X_pca[len(X_real):]\n",
    "\n",
    "pca_distance = float(np.linalg.norm(real_pca.mean(axis=0) - synth_pca.mean(axis=0)))\n",
    "reports[\"PCA_distance\"] = pca_distance\n",
    "\n",
    "# ---------- Classifier Two-Sample Test (CTST) ----------\n",
    "labels = np.concatenate([\n",
    "    np.zeros(len(X_real), dtype=int),\n",
    "    np.ones(len(X_synth), dtype=int)\n",
    "])\n",
    "X_train_ctst, X_test_ctst, y_train_ctst, y_test_ctst = train_test_split(\n",
    "    X_combined, labels, test_size=0.3, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "rf_ctst = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf_ctst.fit(X_train_ctst, y_train_ctst)\n",
    "probs_ctst = rf_ctst.predict_proba(X_test_ctst)[:, 1]\n",
    "auc_ctst = roc_auc_score(y_test_ctst, probs_ctst)\n",
    "reports[\"CTST_AUC\"] = float(auc_ctst)\n",
    "\n",
    "# ---------- Correlation matrix Frobenius norm ----------\n",
    "corr_real = real_fraud[numeric_cols].corr().values\n",
    "corr_synth = synthetic_fraud[numeric_cols].corr().values\n",
    "corr_diff = float(np.linalg.norm(corr_real - corr_synth, ord=\"fro\"))\n",
    "reports[\"Corr_Frobenius\"] = corr_diff\n",
    "\n",
    "# Save validation summary\n",
    "with open(os.path.join(CONFIG[\"REPORTS_DIR\"], \"synthetic_validation_summary.json\"), \"w\") as f:\n",
    "    json.dump(reports, f, indent=2)\n",
    "\n",
    "print(\"[OK] Synthetic validation metrics saved.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) FRAUD DETECTION MODEL: REAL vs REAL+SYNTH\n",
    "# ============================================================\n",
    "print(\"[INFO] Training fraud classifiers (baseline vs synthetic-aug)...\")\n",
    "\n",
    "features = numeric_cols  # Use only numeric for RF\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"isFraud\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# ---- Baseline model: Real only ----\n",
    "rf_real = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf_real.fit(X_train, y_train)\n",
    "probs_real = rf_real.predict_proba(X_test)[:, 1]\n",
    "pred_real = (probs_real >= 0.5).astype(int)\n",
    "\n",
    "metrics_real = {\n",
    "    \"AUC\": roc_auc_score(y_test, probs_real),\n",
    "    \"F1\": f1_score(y_test, pred_real),\n",
    "    \"Precision\": precision_score(y_test, pred_real),\n",
    "    \"Recall\": recall_score(y_test, pred_real),\n",
    "}\n",
    "pd.DataFrame([metrics_real]).to_csv(\n",
    "    os.path.join(CONFIG[\"REPORTS_DIR\"], \"rf_baseline_metrics.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ---- Synthetic-augmented model ----\n",
    "X_synth_aug = synthetic_fraud[features]\n",
    "y_synth_aug = np.ones(len(X_synth_aug), dtype=int)\n",
    "\n",
    "X_train_aug = pd.concat([X_train, X_synth_aug], axis=0).reset_index(drop=True)\n",
    "y_train_aug = pd.concat([y_train, pd.Series(y_synth_aug)], axis=0).reset_index(drop=True)\n",
    "\n",
    "rf_aug = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf_aug.fit(X_train_aug, y_train_aug)\n",
    "probs_aug = rf_aug.predict_proba(X_test)[:, 1]\n",
    "pred_aug = (probs_aug >= 0.5).astype(int)\n",
    "\n",
    "metrics_aug = {\n",
    "    \"AUC\": roc_auc_score(y_test, probs_aug),\n",
    "    \"F1\": f1_score(y_test, pred_aug),\n",
    "    \"Precision\": precision_score(y_test, pred_aug),\n",
    "    \"Recall\": recall_score(y_test, pred_aug),\n",
    "}\n",
    "pd.DataFrame([metrics_aug]).to_csv(\n",
    "    os.path.join(CONFIG[\"REPORTS_DIR\"], \"rf_synth_aug_metrics.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"[OK] Fraud detection metrics (baseline vs augmented) saved.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) ANOMALY DETECTION WITH ISOLATION FOREST\n",
    "# ============================================================\n",
    "print(\"[INFO] Training IsolationForest anomaly detector...\")\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.03,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_legit = legit_df[features]\n",
    "iso.fit(X_legit)\n",
    "\n",
    "scores_test = -iso.score_samples(X_test)  # higher = more anomalous\n",
    "auc_iso = roc_auc_score(y_test, scores_test)\n",
    "\n",
    "anomaly_report = {\n",
    "    \"IsolationForest_AUC\": float(auc_iso)\n",
    "}\n",
    "with open(os.path.join(CONFIG[\"REPORTS_DIR\"], \"anomaly_iforest_metrics.json\"), \"w\") as f:\n",
    "    json.dump(anomaly_report, f, indent=2)\n",
    "\n",
    "print(\"[OK] Anomaly detection metrics saved.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) DISTRIBUTION PLOTS FOR TOP FEATURES\n",
    "# ============================================================\n",
    "print(\"[INFO] Saving KDE distribution plots for top numeric features...\")\n",
    "\n",
    "# Take features with lowest KS (closest real vs synth)\n",
    "top_for_plots = ks_ws_df.sort_values(\"KS\").head(6)[\"Feature\"].tolist()\n",
    "\n",
    "for col in top_for_plots:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.kdeplot(real_fraud[col], label=\"Real Fraud\", fill=True, alpha=0.4)\n",
    "    sns.kdeplot(synthetic_fraud[col], label=\"Synthetic Fraud\", fill=True, alpha=0.4)\n",
    "    plt.title(f\"Real vs Synthetic ‚Äî {col}\")\n",
    "    plt.legend()\n",
    "    out_path = os.path.join(CONFIG[\"PLOTS_DIR\"], f\"{col}_kde.png\")\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"üéâ DONE. FRAUDGEN v3 PIPELINE FINISHED SUCCESSFULLY.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63cfbc4-23eb-4474-bf9c-7de07d848fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud count = 20663\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Processed/train_preprocessed.csv\")\n",
    "print(\"Fraud count =\", df[df[\"isFraud\"]==1].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e4d2a0-fbc1-4108-8e08-f3d133055a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud count = 200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Synthetic/synthetic_fraud_v3.csv\")\n",
    "print(\"Fraud count =\", df[df[\"isFraud\"]==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cc00a8e-5500-4142-82a8-4ba537a7cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] CSV not found: .\\synthetic_data_validation_summary.csv (skipping)\n",
      "[WARN] JSON not found: .\\synthetic_validation_summary.json (skipping)\n",
      "[WARN] CSV not found: .\\validation_ks_wasserstein.csv (skipping)\n",
      "[WARN] CSV not found: .\\pearson_feature_ranking.csv (skipping)\n",
      "[WARN] CSV not found: .\\rf_baseline_metrics.csv (skipping)\n",
      "[WARN] CSV not found: .\\rf_synth_aug_metrics.csv (skipping)\n",
      "[WARN] CSV not found: .\\performance_metrics_ctgan.csv (skipping)\n",
      "[WARN] JSON not found: .\\anomaly_iforest_metrics.json (skipping)\n",
      "[INFO] Creating PDF report -> .\\FraudGen_Performance_Report.pdf\n",
      "[OK] Report saved -> .\\FraudGen_Performance_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FraudGen ‚Äì Performance & Synthetic Data Validation Report Generator\n",
    "-------------------------------------------------------------------\n",
    "This script:\n",
    "  - Loads existing CSV/JSON metric files from your project\n",
    "  - Creates modern-style matplotlib plots\n",
    "  - Generates a multi-page PDF report:\n",
    "      * Title & summary page\n",
    "      * Synthetic data quality\n",
    "      * Model performance (baseline vs synthetic-augmented)\n",
    "      * Anomaly detection (Isolation Forest)\n",
    "  - Output: FraudGen_Performance_Report.pdf\n",
    "\n",
    "Run inside your `fraudgen` conda env:\n",
    "\n",
    "    python generate_fraudgen_report.py\n",
    "\n",
    "You can tweak paths in the CONFIG block if your files live elsewhere.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG ‚Äì EDIT PATHS HERE IF NEEDED\n",
    "# =====================================================\n",
    "\n",
    "CONFIG = {\n",
    "    \"BASE_DIR\": \".\",  # project root\n",
    "    \"OUTPUT_PDF\": \"FraudGen_Performance_Report.pdf\",\n",
    "\n",
    "    # metrics files\n",
    "    \"ANOMALY_JSON\": \"anomaly_iforest_metrics.json\",\n",
    "    \"SYNTH_VAL_JSON\": \"synthetic_validation_summary.json\",  # optional\n",
    "    \"SYNTH_VAL_CSV\": \"synthetic_data_validation_summary.csv\",\n",
    "    \"KS_WS_CSV\": \"validation_ks_wasserstein.csv\",\n",
    "    \"PEARSON_FEATS_CSV\": \"pearson_feature_ranking.csv\",\n",
    "    \"RF_BASELINE_CSV\": \"rf_baseline_metrics.csv\",\n",
    "    \"RF_SYNTH_CSV\": \"rf_synth_aug_metrics.csv\",\n",
    "    \"CTGAN_METRICS_CSV\": \"performance_metrics_ctgan.csv\",  # optional\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# HELPER ‚Äì MODERN MATPLOTLIB STYLE\n",
    "# =====================================================\n",
    "\n",
    "def set_modern_style():\n",
    "    plt.style.use(\"default\")\n",
    "    plt.rcParams.update({\n",
    "        \"figure.facecolor\": \"white\",\n",
    "        \"axes.facecolor\": \"#f7f7f7\",\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"axes.edgecolor\": \"#cccccc\",\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"axes.titlesize\": 12,\n",
    "        \"axes.labelsize\": 10,\n",
    "        \"xtick.labelsize\": 9,\n",
    "        \"ytick.labelsize\": 9,\n",
    "        \"legend.fontsize\": 9,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"DejaVu Sans\", \"Arial\", \"Helvetica\"],\n",
    "    })\n",
    "\n",
    "\n",
    "# nice palette for modern look (you asked for this)\n",
    "COLORS = {\n",
    "    \"baseline\": \"#1f77b4\",\n",
    "    \"synth\": \"#ff7f0e\",\n",
    "    \"other\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# SAFE LOADERS\n",
    "# =====================================================\n",
    "\n",
    "def load_csv_safe(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] CSV not found: {path} (skipping)\")\n",
    "        return None\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read CSV {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_json_safe(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] JSON not found: {path} (skipping)\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read JSON {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# =====================================================\n",
    "# 1) TITLE & SUMMARY PAGE\n",
    "# =====================================================\n",
    "\n",
    "def add_title_page(pdf, synth_summary_df, ctgan_metrics_df):\n",
    "    set_modern_style()\n",
    "    fig, ax = plt.subplots(figsize=(8.27, 11.69))  # A4-ish\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    title = \"FraudGen: Synthetic Data and Anomaly Detection\\nfor Banking Transactions\"\n",
    "    subtitle = \"Performance, Synthetic Data Validation, and Anomaly Detection Report\"\n",
    "\n",
    "    y = 0.9\n",
    "    ax.text(0.5, y, title, ha=\"center\", va=\"top\", fontsize=18, weight=\"bold\")\n",
    "    y -= 0.05\n",
    "    ax.text(0.5, y, subtitle, ha=\"center\", va=\"top\", fontsize=11, color=\"#555555\")\n",
    "\n",
    "    y -= 0.08\n",
    "\n",
    "    # Synthetic quality quick summary (if available)\n",
    "    if synth_summary_df is not None and not synth_summary_df.empty:\n",
    "        summary = synth_summary_df.iloc[0].to_dict()\n",
    "        lines = []\n",
    "        if \"Mean_KS\" in summary:\n",
    "            lines.append(f\"Mean KS Statistic (numeric features): {summary['Mean_KS']:.4f}\")\n",
    "        if \"Mean_Wasserstein\" in summary:\n",
    "            lines.append(f\"Mean Wasserstein Distance: {summary['Mean_Wasserstein']:.4f}\")\n",
    "        if \"CTST_AUC\" in summary:\n",
    "            lines.append(f\"Classifier Two-Sample Test AUC: {summary['CTST_AUC']:.3f}\")\n",
    "        if \"TSTR_F1\" in summary:\n",
    "            lines.append(f\"TSTR F1 (train synthetic, test real): {summary['TSTR_F1']:.3f}\")\n",
    "\n",
    "        if lines:\n",
    "            ax.text(0.1, y, \"Synthetic Data Quality (Summary):\", fontsize=11, weight=\"bold\")\n",
    "            y -= 0.03\n",
    "            for line in lines:\n",
    "                ax.text(0.12, y, \"‚Ä¢ \" + line, fontsize=10)\n",
    "                y -= 0.025\n",
    "\n",
    "    # CTGAN training stats, if present\n",
    "    if ctgan_metrics_df is not None and not ctgan_metrics_df.empty:\n",
    "        ct = ctgan_metrics_df.iloc[0].to_dict()\n",
    "        lines = []\n",
    "        for k in [\"epochs\", \"batch_size\", \"train_time_minutes\"]:\n",
    "            if k in ct:\n",
    "                lines.append(f\"{k.replace('_', ' ').title()}: {ct[k]}\")\n",
    "        if lines:\n",
    "            y -= 0.03\n",
    "            ax.text(0.1, y, \"CTGAN Training Summary:\", fontsize=11, weight=\"bold\")\n",
    "            y -= 0.03\n",
    "            for line in lines:\n",
    "                ax.text(0.12, y, \"‚Ä¢ \" + str(line), fontsize=10)\n",
    "                y -= 0.025\n",
    "\n",
    "    # High-level narrative block\n",
    "    y -= 0.04\n",
    "    paragraph = (\n",
    "        \"This report summarizes the performance of the FraudGen system, which generates \"\n",
    "        \"synthetic fraudulent banking transactions using a CTGAN-based model and evaluates \"\n",
    "        \"their impact on downstream fraud and anomaly detection. The analysis compares base \"\n",
    "        \"models trained on imbalanced real data against models trained or augmented with \"\n",
    "        \"synthetic fraud, and reports gains or trade-offs in F1, precision, recall, and \"\n",
    "        \"anomaly detection performance.\"\n",
    "    )\n",
    "    wrapped = textwrap.fill(paragraph, width=90)\n",
    "    ax.text(0.1, y, wrapped, fontsize=10, va=\"top\")\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2) SYNTHETIC DATA QUALITY PLOTS\n",
    "# =====================================================\n",
    "\n",
    "def add_ks_wasserstein_page(pdf, ks_df):\n",
    "    if ks_df is None or ks_df.empty:\n",
    "        return\n",
    "\n",
    "    set_modern_style()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Expect columns like: Feature, KS_Statistic, Wasserstein_Distance\n",
    "    cols = ks_df.columns.str.lower()\n",
    "    ks_col = None\n",
    "    ws_col = None\n",
    "    for c in ks_df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"ks\" in cl and \"stat\" in cl:\n",
    "            ks_col = c\n",
    "        if \"wasser\" in cl:\n",
    "            ws_col = c\n",
    "    if ks_col is None and \"KS\" in ks_df.columns:\n",
    "        ks_col = \"KS\"\n",
    "    if ws_col is None and \"Wasserstein\" in ks_df.columns:\n",
    "        ws_col = \"Wasserstein\"\n",
    "\n",
    "    text_lines = []\n",
    "    if ks_col is not None:\n",
    "        ks_vals = ks_df[ks_col].astype(float)\n",
    "        text_lines.append(f\"Mean KS statistic: {ks_vals.mean():.4f}\")\n",
    "        text_lines.append(f\"Median KS statistic: {ks_vals.median():.4f}\")\n",
    "        ax.hist(ks_vals, bins=30, color=COLORS[\"baseline\"], alpha=0.8)\n",
    "        ax.set_title(\"Distribution of KS Statistics Across Features\")\n",
    "        ax.set_xlabel(\"KS Statistic\")\n",
    "        ax.set_ylabel(\"Feature Count\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"KS statistics column not found.\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 5))\n",
    "    if ws_col is not None:\n",
    "        ws_vals = ks_df[ws_col].astype(float)\n",
    "        text_lines.append(f\"Mean Wasserstein distance: {ws_vals.mean():.4f}\")\n",
    "        text_lines.append(f\"Median Wasserstein distance: {ws_vals.median():.4f}\")\n",
    "        ax2.hist(ws_vals, bins=30, color=COLORS[\"synth\"], alpha=0.8)\n",
    "        ax2.set_title(\"Distribution of Wasserstein Distances Across Features\")\n",
    "        ax2.set_xlabel(\"Wasserstein Distance\")\n",
    "        ax2.set_ylabel(\"Feature Count\")\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"Wasserstein column not found.\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Add text summary at bottom of first fig\n",
    "    if text_lines:\n",
    "        fig.subplots_adjust(bottom=0.25)\n",
    "        y0 = 0.02\n",
    "        for line in text_lines:\n",
    "            fig.text(0.01, y0, line, fontsize=9)\n",
    "            y0 += 0.03\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "def add_top_feature_corr_page(pdf, pearson_df, top_k=10):\n",
    "    if pearson_df is None or pearson_df.empty:\n",
    "        return\n",
    "\n",
    "    # Expect something like: Feature, Pearson_Corr\n",
    "    cols = pearson_df.columns.str.lower()\n",
    "    feat_col = pearson_df.columns[0]\n",
    "    pearson_col = pearson_df.columns[1] if len(pearson_df.columns) > 1 else None\n",
    "\n",
    "    for c in pearson_df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"feature\" in cl or \"name\" in cl:\n",
    "            feat_col = c\n",
    "        if \"pearson\" in cl or \"corr\" in cl:\n",
    "            pearson_col = c\n",
    "\n",
    "    if pearson_col is None:\n",
    "        return\n",
    "\n",
    "    df = pearson_df.copy()\n",
    "    df[pearson_col] = df[pearson_col].astype(float).abs()\n",
    "    df_top = df.sort_values(pearson_col, ascending=False).head(top_k)\n",
    "\n",
    "    set_modern_style()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.barh(df_top[feat_col], df_top[pearson_col], color=COLORS[\"other\"])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"Top {top_k} Features by |Pearson Correlation| with isFraud\")\n",
    "    ax.set_xlabel(\"Absolute Pearson Correlation\")\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3) MODEL PERFORMANCE COMPARISON\n",
    "# =====================================================\n",
    "\n",
    "def add_model_performance_page(pdf, baseline_df, synth_df):\n",
    "    if baseline_df is None or baseline_df.empty or synth_df is None or synth_df.empty:\n",
    "        return\n",
    "\n",
    "    # Assume single-row CSVs with metrics like F1, Precision, Recall, AUC\n",
    "    base = baseline_df.iloc[0].to_dict()\n",
    "    synth = synth_df.iloc[0].to_dict()\n",
    "\n",
    "    # pick common numeric metrics\n",
    "    metrics = []\n",
    "    for m in [\"F1\", \"F1_score\", \"f1\", \"Precision\", \"precision\", \"Recall\", \"recall\", \"AUC\", \"roc_auc\"]:\n",
    "        if m in base and m in synth:\n",
    "            metrics.append(m)\n",
    "    metrics = list(dict.fromkeys(metrics))  # unique & ordered\n",
    "\n",
    "    if not metrics:\n",
    "        print(\"[WARN] No common numeric metrics between baseline and synth metrics.\")\n",
    "        return\n",
    "\n",
    "    set_modern_style()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    base_vals = [float(base[m]) for m in metrics]\n",
    "    synth_vals = [float(synth[m]) for m in metrics]\n",
    "\n",
    "    ax.bar(x - width/2, base_vals, width, label=\"Real-only Baseline\", color=COLORS[\"baseline\"], alpha=0.9)\n",
    "    ax.bar(x + width/2, synth_vals, width, label=\"Real + Synthetic (FraudGen)\", color=COLORS[\"synth\"], alpha=0.9)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_ylim(0, max(base_vals + synth_vals) * 1.15)\n",
    "    ax.set_title(\"Model Performance: Real vs Real+Synthetic (FraudGen)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # annotate bars\n",
    "    for i, (bx, sx) in enumerate(zip(base_vals, synth_vals)):\n",
    "        ax.text(x[i] - width/2, bx + 0.01, f\"{bx:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "        ax.text(x[i] + width/2, sx + 0.01, f\"{sx:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) ANOMALY DETECTION PAGE (ISOLATION FOREST)\n",
    "# =====================================================\n",
    "\n",
    "def add_anomaly_page(pdf, anomaly_json):\n",
    "    if anomaly_json is None:\n",
    "        return\n",
    "\n",
    "    # Assume anomaly_json is dict of metric_name -> value\n",
    "    metrics = {k: v for k, v in anomaly_json.items() if isinstance(v, (int, float))}\n",
    "    if not metrics:\n",
    "        return\n",
    "\n",
    "    names = list(metrics.keys())\n",
    "    vals = [float(metrics[k]) for k in names]\n",
    "\n",
    "    set_modern_style()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(len(names))\n",
    "    ax.bar(x, vals, color=COLORS[\"other\"], alpha=0.9)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names, rotation=30, ha=\"right\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Anomaly Detection Performance (Isolation Forest)\")\n",
    "\n",
    "    for i, v in enumerate(vals):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) MAIN\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    base = CONFIG[\"BASE_DIR\"]\n",
    "\n",
    "    # Load everything\n",
    "    synth_summary_csv = load_csv_safe(os.path.join(base, CONFIG[\"SYNTH_VAL_CSV\"]))\n",
    "    synth_val_json = load_json_safe(os.path.join(base, CONFIG[\"SYNTH_VAL_JSON\"]))\n",
    "    ks_ws_df = load_csv_safe(os.path.join(base, CONFIG[\"KS_WS_CSV\"]))\n",
    "    pearson_df = load_csv_safe(os.path.join(base, CONFIG[\"PEARSON_FEATS_CSV\"]))\n",
    "    rf_base_df = load_csv_safe(os.path.join(base, CONFIG[\"RF_BASELINE_CSV\"]))\n",
    "    rf_synth_df = load_csv_safe(os.path.join(base, CONFIG[\"RF_SYNTH_CSV\"]))\n",
    "    ctgan_df = load_csv_safe(os.path.join(base, CONFIG[\"CTGAN_METRICS_CSV\"]))\n",
    "    anomaly_json = load_json_safe(os.path.join(base, CONFIG[\"ANOMALY_JSON\"]))\n",
    "\n",
    "    # If JSON version of synthetic summary exists but CSV doesn't have those columns,\n",
    "    # you can enhance later. For now, we mainly use CSV.\n",
    "\n",
    "    out_path = os.path.join(base, CONFIG[\"OUTPUT_PDF\"])\n",
    "    print(f\"[INFO] Creating PDF report -> {out_path}\")\n",
    "\n",
    "    with PdfPages(out_path) as pdf:\n",
    "        # 1) Title page\n",
    "        add_title_page(pdf, synth_summary_csv, ctgan_df)\n",
    "\n",
    "        # 2) Synthetic data quality pages: KS & Wasserstein + feature correlations\n",
    "        add_ks_wasserstein_page(pdf, ks_ws_df)\n",
    "        add_top_feature_corr_page(pdf, pearson_df, top_k=10)\n",
    "\n",
    "        # 3) Model performance baseline vs synthetic\n",
    "        add_model_performance_page(pdf, rf_base_df, rf_synth_df)\n",
    "\n",
    "        # 4) Anomaly detection metrics\n",
    "        add_anomaly_page(pdf, anomaly_json)\n",
    "\n",
    "        # 5) Final short conclusions page\n",
    "        set_modern_style()\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.axis(\"off\")\n",
    "        txt = \"\"\"\n",
    "        Conclusions\n",
    "\n",
    "        ‚Ä¢ Synthetic fraud data generated by FraudGen was integrated into the model\n",
    "          training pipeline and evaluated against a real-data-only baseline.\n",
    "        ‚Ä¢ The validation metrics (KS, Wasserstein, and classifier two-sample tests)\n",
    "          show the degree of alignment between real and synthetic distributions,\n",
    "          while TSTR gives a task-level measure of utility.\n",
    "        ‚Ä¢ The comparison between baseline and synthetic-augmented models shows how\n",
    "          synthetic data affects F1, precision, recall, and AUC on real held-out data.\n",
    "        ‚Ä¢ Anomaly detection metrics (Isolation Forest) indicate whether synthetic\n",
    "          data helps the model learn more robust decision boundaries for rare fraud.\n",
    "        \n",
    "        This report can now be cited as the quantitative backbone of the FraudGen\n",
    "        project, and the figures can be reused in your final course report, slides,\n",
    "        or a future conference/journal submission.\n",
    "        \"\"\"\n",
    "        ax.text(0.05, 0.95, \"Summary & Conclusions\", fontsize=14, weight=\"bold\", va=\"top\")\n",
    "        ax.text(0.05, 0.9, textwrap.fill(txt, width=90), fontsize=10, va=\"top\")\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] Report saved -> {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dda4d8b-4aa5-4b58-a934-676b5a17e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using KS column = 'KS'\n",
      "[INFO] BEST MATCHED FEATURES: ['TransactionDT', 'TransactionID', 'card2', 'card1', 'D4', 'addr1', 'TransactionAmt', 'C6']\n",
      "[DONE] Plots saved to folder: Plots\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# LOAD VALIDATION + DATA\n",
    "# ===============================\n",
    "ks_path = \"Reports/validation_ks_wasserstein.csv\"\n",
    "real_path = \"Processed/train_preprocessed.csv\"\n",
    "synthetic_path = \"Synthetic/synthetic_fraud_v3.csv\"  # change if name differs\n",
    "\n",
    "ks_df = pd.read_csv(ks_path)\n",
    "real_df = pd.read_csv(real_path)\n",
    "synthetic_df = pd.read_csv(synthetic_path)\n",
    "\n",
    "# ===============================\n",
    "# FIX COLUMN NAME FOR KS\n",
    "# ===============================\n",
    "\n",
    "# Try to find correct KS column even if mislabeled\n",
    "possible_names = [\"KS\", \"KS_Statistic\", \"Kolmogorov_Smirnov\", \"KS Value\", \"KS_value\"]\n",
    "\n",
    "ks_col = None\n",
    "for col in ks_df.columns:\n",
    "    if col in possible_names:\n",
    "        ks_col = col\n",
    "        break\n",
    "\n",
    "# If no match, detect numeric column automatically\n",
    "if ks_col is None:\n",
    "    numeric_cols = ks_df.select_dtypes(include=\"number\").columns\n",
    "    if len(numeric_cols) >= 1:\n",
    "        ks_col = numeric_cols[0]\n",
    "        print(f\"[WARN] Using numeric column '{ks_col}' as KS statistic\")\n",
    "    else:\n",
    "        raise KeyError(\"‚ÄºÔ∏è ERROR: No numeric KS column found in validation file.\")\n",
    "\n",
    "print(f\"[OK] Using KS column = '{ks_col}'\")\n",
    "\n",
    "# ===============================\n",
    "# PICK TOP FEATURES (LOW KS = GOOD MATCH)\n",
    "# ===============================\n",
    "ks_df_sorted = ks_df.sort_values(by=ks_col)\n",
    "top_features = ks_df_sorted[\"Feature\"].head(8).tolist()\n",
    "\n",
    "print(\"[INFO] BEST MATCHED FEATURES:\", top_features)\n",
    "\n",
    "# ===============================\n",
    "# GENERATE KDE PLOTS\n",
    "# ===============================\n",
    "\n",
    "save_dir = \"Plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for col in top_features:\n",
    "    if col not in real_df.columns or col not in synthetic_df.columns:\n",
    "        print(f\"[SKIP] Missing column: {col}\")\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.kdeplot(real_df[col].dropna(), fill=True, label=\"Real\", alpha=0.45)\n",
    "    sns.kdeplot(synthetic_df[col].dropna(), fill=True, label=\"Synthetic\", alpha=0.45)\n",
    "    plt.title(f\"KDE Comparison: {col}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/kde_{col}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"[DONE] Plots saved to folder: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "025a4cfa-42a7-4b62-bf3e-bfb7d7777bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Histogram comparison plots saved in /Plots folder\n",
      "[OK] Correlation heatmaps saved\n",
      "[OK] Fixed PCA plot generated successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# =====================================================\n",
    "# LOAD DATA (CHANGE PATHS IF NEEDED)\n",
    "# =====================================================\n",
    "real_df = pd.read_csv(\"Processed/train_preprocessed.csv\")\n",
    "synthetic_df = pd.read_csv(\"Synthetic/synthetic_fraud_v3.csv\")\n",
    "\n",
    "# Filter real fraud\n",
    "real_fraud = real_df[real_df[\"isFraud\"] == 1]\n",
    "\n",
    "# Align common columns\n",
    "common_cols = [c for c in real_fraud.columns if c in synthetic_df.columns and c != \"isFraud\"]\n",
    "real_fraud = real_fraud[common_cols]\n",
    "synthetic_df = synthetic_df[common_cols]\n",
    "\n",
    "# =====================================================\n",
    "# 1) HISTOGRAM OVERLAY COMPARISON\n",
    "# =====================================================\n",
    "numeric_cols = real_fraud.select_dtypes(include=[np.number]).columns[:8]  # top 8 for clarity\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(real_fraud[col], color=\"blue\", kde=True, stat=\"density\", label=\"Real\", alpha=0.5)\n",
    "    sns.histplot(synthetic_df[col], color=\"orange\", kde=True, stat=\"density\", label=\"Synthetic\", alpha=0.5)\n",
    "    plt.title(f\"Distribution Comparison: {col}\", fontsize=14, weight='bold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/hist_{col}.png\", dpi=350)\n",
    "    plt.close()\n",
    "\n",
    "print(\"[OK] Histogram comparison plots saved in /Plots folder\")\n",
    "\n",
    "# =====================================================\n",
    "# 2) CORRELATION HEATMAP COMPARISON\n",
    "# =====================================================\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.heatmap(real_fraud.corr(), cmap=\"coolwarm\", center=0, annot=False)\n",
    "plt.title(\"Real Fraud Correlation Heatmap\", fontsize=14, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/heatmap_real.png\", dpi=350)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.heatmap(synthetic_df.corr(), cmap=\"coolwarm\", center=0, annot=False)\n",
    "plt.title(\"Synthetic Fraud Correlation Heatmap\", fontsize=14, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/heatmap_synth.png\", dpi=350)\n",
    "plt.close()\n",
    "\n",
    "print(\"[OK] Correlation heatmaps saved\")\n",
    "\n",
    "# =====================================================\n",
    "# 3) PCA DIMENSIONAL REDUCTION (SCATTER PLOT) [FIXED]\n",
    "# =====================================================\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_real = real_fraud.select_dtypes(include=[np.number])\n",
    "numeric_synth = synthetic_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "scaled_real = scaler.fit_transform(numeric_real.fillna(0))\n",
    "scaled_synth = scaler.transform(numeric_synth.fillna(0))\n",
    "\n",
    "# PCA Projection\n",
    "pca = PCA(n_components=2)\n",
    "pca_real = pca.fit_transform(scaled_real)\n",
    "pca_synth = pca.transform(scaled_synth)\n",
    "\n",
    "# Plotting PCA comparison\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pca_real[:, 0], pca_real[:, 1], alpha=0.4, s=10, label=\"Real Fraud\", color=\"blue\")\n",
    "plt.scatter(pca_synth[:, 0], pca_synth[:, 1], alpha=0.4, s=10, label=\"Synthetic Fraud\", color=\"orange\")\n",
    "\n",
    "plt.title(\"PCA Comparison: Real vs Synthetic Fraud\", fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/pca_comparison.png\", dpi=350)\n",
    "plt.close()\n",
    "\n",
    "print(\"[OK] Fixed PCA plot generated successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83223a-0a75-4dfc-9f20-1f98af10f050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fraudgen]",
   "language": "python",
   "name": "conda-env-fraudgen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
